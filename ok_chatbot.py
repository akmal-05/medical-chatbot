# -*- coding: utf-8 -*-
"""OK-ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pIGjKnv68g211BzfbXjuXmRAkCmXoZ6k
"""

from google.colab import drive
drive.mount('/content/drive')

pip install gradio groq

import json
import re
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import gradio as gr
from groq import Groq

# Load JSON Dataset
def load_json_dataset(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
    intents = data["intents"]
    questions, answers = [], []
    for intent in intents:
        for pattern in intent["patterns"]:
            questions.append(pattern)
            answers.append(" ".join(intent["responses"]))
    return questions, answers

# Preprocessing function
def preprocess(text):
    text = text.lower()
    text = re.sub(r"[.!?]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

# Load dataset
file_path = "/content/drive/MyDrive/OK-ChatBot/intents.json"  # Update the path
questions, answers = load_json_dataset(file_path)

# Preprocess questions
questions = [preprocess(q) for q in questions]

# Load first model (Existing Medical Chatbot)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(questions))

# Create embeddings for all questions
with torch.no_grad():
    question_embeddings = model(**tokenizer(questions, padding=True, truncation=True, return_tensors="pt")).logits

# First Model: Get response from the initial chatbot
def get_response(user_input):
    user_input = preprocess(user_input)
    with torch.no_grad():
        user_embedding = model(**tokenizer([user_input], padding=True, truncation=True, return_tensors="pt")).logits
    similarities = torch.cosine_similarity(user_embedding, question_embeddings, dim=1)
    best_match_index = similarities.argmax().item()
    return answers[best_match_index]

# Initialize the Groq client with your API key
client = Groq(
    api_key="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",
)

# Define a function to get a response using Groq API
def get_groq_response(user_input):
    try:
        # Create a chat completion request
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": "You are a medical expert. Only answer medical-related questions. If the question is unrelated to the medical field, respond with 'This is outside the scope of medical knowledge.'",
                },
                {
                    "role": "user",
                    "content": user_input,
                },
            ],
            model="llama3-8b-8192",  # Ensure this model is supported by Groq
        )
        # Extract the first choice's message content
        assistant_message = chat_completion.choices[0].message.content
        return assistant_message
    except AttributeError as e:
        return "Error in accessing the chat completion response: " + str(e)
    except Exception as e:
        return f"An error occurred: {str(e)}"

# Combined Chatbot Functionality
def combined_chatbot(user_input):
    # Step 1: Get response from the first model
    primary_response = get_response(user_input)
    # Step 2: Validate or refine response with the second model
    validated_response = get_groq_response(user_input)
    return f" Assistant: {validated_response}"

# Gradio Interface
interface = gr.Interface(
    fn=combined_chatbot,
    inputs=gr.Textbox(label="Your Medical Query", placeholder="Type your question here..."),
    outputs=gr.Textbox(label="Chatbot Response"),
    title="AI-Powered Medical Chatbot",
    description="Ask questions about diseases, symptoms, or prevention. The system will provide medical-specific responses or indicate if the question is out of scope.",
)

# Launch the Gradio Interface
interface.launch()

